{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ebe1378",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "23ad2a34",
     "kernelId": ""
    }
   },
   "source": [
    "# Quellen: \n",
    "\n",
    "[Google Github, o. D.] - Google (o. D.). GitHub - google/sentencepiece: Unsupervised text tokenizer for Neural Network-based text generation. GitHub. [online] https://github.com/google/sentencepiece [abgerufen am 16.01.2022].\n",
    "\n",
    "[Jitsi, o. D.] - Jitsi (o. D.). GitHub - jitsi/jiwer: Evaluate your speech-to-text system with similarity measures such as word error rate (WER). GitHub. [online] https://github.com/jitsi/jiwer [abgerufen am 17.01.2022].\n",
    "\n",
    "[Klein et al., 2018] - Klein, Guillaume/Kim, Yoon/Deng, Yuntian/Nguyen, Vincent/Senellart, Jean/Rush, Alexander M. (2018). OpenNMT: Neural Machine Translation Toolkit. In: Proceedings of AMTA 2018, Vol. 1, MT Research Track, 177-184. [online] https://aclanthology.org/W18-1817.pdf [abgerufen am 30.12.21].\n",
    "\n",
    "[Koehn, 2005] - Koehn, Philipp (2005). Europarl: A parallel corpus for statistical machine translation. In: MT Summit, 5, 79-86. [online] https://homepages.inf.ed.ac.uk/pkoehn/publications/europarl-mtsummit05.pdf [abgerufen am 04.01.2022]\n",
    "\n",
    "[Mjpost, o. D.] - Mjpost (o. D.). GitHub - mjpost/sacrebleu: Reference BLEU implementation that auto-downloads test sets and reports a version string to facilitate cross-lab comparisons. GitHub. [online] https://github.com/mjpost/sacrebleu [abgerufen am 17.01.2022].\n",
    "\n",
    "[OpenNMT, o. D.a] - Train — OpenNMT-py documentation (o. D.). OpenNMT. [online] https://opennmt.net/OpenNMT-py/options/train.html [abgerufen am 01.01.2022].\n",
    "\n",
    "[OpenNMT, o. D.b] - OpenNMT (o. D.). GitHub - OpenNMT/OpenNMT-py: Open Source Neural Machine Translation in PyTorch. GitHub. [online] https://github.com/OpenNMT/OpenNMT-py [abgerufen am 04.01.2022].\n",
    "\n",
    "[OpenNMT, o. D.c] - Translation — OpenNMT-py documentation (o. D.). OpenNMT. [online] https://opennmt.net/OpenNMT-py/examples/Translation.html [abgerufen am 01.01.2022].\n",
    "\n",
    "[OPUS, o. D.] - Europarl (o. D.). OPUS. [online] https://opus.nlpl.eu/Europarl.php [abgerufen am 01.01.2022].\n",
    "\n",
    "[Tiedemann, 2012] - Tiedemann, Jörg (2012). Parallel Data, Tools and Interfaces in OPUS. In: Proceedings of the 8th International Conference on Language Resources and Evaluation (LREC 2012). [online] http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf [abgerufen am 04.01.2022].\n",
    "\n",
    "[Ymoslem, o. D.a] - Ymoslem (o. D.). GitHub - ymoslem/MT-Preparation: Machine Translation (MT) Preparation Scripts. GitHub. [online] https://github.com/ymoslem/MT-Preparation [abgerufen am 01.01.2022].\n",
    "\n",
    "[Ymoslem, o. D.b] - Ymoslem (o. D.). GitHub - ymoslem/MT-Evaluation: Machine Translation (MT) Evaluation Scripts. GitHub. [online] https://github.com/ymoslem/MT-Evaluation [abgerufen am 04.01.2022]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8715b0d",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "dcfa0704",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting OpenNMT-py\n",
      "  Downloading OpenNMT_py-2.2.0-py3-none-any.whl (216 kB)\n",
      "\u001b[K     |████████████████████████████████| 216 kB 15.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from OpenNMT-py) (1.10.0a0+0aef44c)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.8/site-packages (from OpenNMT-py) (5.4.1)\n",
      "Requirement already satisfied: flask in /opt/conda/lib/python3.8/site-packages (from OpenNMT-py) (2.0.2)\n",
      "Collecting torchtext==0.5.0\n",
      "  Downloading torchtext-0.5.0-py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 34.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting configargparse\n",
      "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: tensorboard>=2.3 in /opt/conda/lib/python3.8/site-packages (from OpenNMT-py) (2.6.0)\n",
      "Collecting waitress\n",
      "  Downloading waitress-2.1.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 56.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyonmttok<2,>=1.23\n",
      "  Downloading pyonmttok-1.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.4 MB 25.0 MB/s eta 0:00:01    |███████████▉                    | 6.1 MB 25.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (2.26.0)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (4.62.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (1.16.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from torchtext==0.5.0->OpenNMT-py) (1.21.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.14.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.8.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.37.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (2.0.2)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (3.18.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (0.6.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.35.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (1.41.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard>=2.3->OpenNMT-py) (58.2.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (4.2.4)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.3->OpenNMT-py) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->torchtext==0.5.0->OpenNMT-py) (1.26.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.3->OpenNMT-py) (3.1.1)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.8/site-packages (from torch>=1.6.0->OpenNMT-py) (3.10.0.2)\n",
      "Requirement already satisfied: click>=7.1.2 in /opt/conda/lib/python3.8/site-packages (from flask->OpenNMT-py) (8.0.1)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /opt/conda/lib/python3.8/site-packages (from flask->OpenNMT-py) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.0 in /opt/conda/lib/python3.8/site-packages (from flask->OpenNMT-py) (2.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.8/site-packages (from Jinja2>=3.0->flask->OpenNMT-py) (2.0.1)\n",
      "Installing collected packages: sentencepiece, waitress, torchtext, pyonmttok, configargparse, OpenNMT-py\n",
      "  Attempting uninstall: torchtext\n",
      "    Found existing installation: torchtext 0.11.0a0\n",
      "    Uninstalling torchtext-0.11.0a0:\n",
      "      Successfully uninstalled torchtext-0.11.0a0\n",
      "Successfully installed OpenNMT-py-2.2.0 configargparse-1.5.3 pyonmttok-1.31.0 sentencepiece-0.1.96 torchtext-0.5.0 waitress-2.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installiert OpenNMT-PyTorch\n",
    "!pip install OpenNMT-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88acf56b",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "5fdf64b2",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (1.21.2)\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.7 MB 20.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 3)) (0.0.46)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (0.1.96)\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[K     |████████████████████████████████| 90 kB 42.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jiwer\n",
      "  Downloading jiwer-2.3.0-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (3.6.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 2)) (2021.3)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 3)) (2021.10.8)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 3)) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 3)) (4.62.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 3)) (1.16.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from sacremoses->-r requirements.txt (line 3)) (1.1.0)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.8/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.4.4)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.8/site-packages (from sacrebleu->-r requirements.txt (line 5)) (0.8.9)\n",
      "Requirement already satisfied: portalocker in /opt/conda/lib/python3.8/site-packages (from sacrebleu->-r requirements.txt (line 5)) (2.3.2)\n",
      "Collecting python-Levenshtein==0.12.2\n",
      "  Downloading python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 55.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from python-Levenshtein==0.12.2->jiwer->-r requirements.txt (line 6)) (58.2.0)\n",
      "Building wheels for collected packages: python-Levenshtein\n",
      "  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-linux_x86_64.whl size=184888 sha256=5837f503b5a1d316296394335c1c7b7af04d5259f97bf313eec81cfd715d7819\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-3fxsf0s3/wheels/d7/0c/76/042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
      "Successfully built python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein, sacrebleu, pandas, jiwer\n",
      "\u001b[33m  WARNING: The script sacrebleu is installed in '/root/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed jiwer-2.3.0 pandas-1.4.1 python-Levenshtein-0.12.2 sacrebleu-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Installiert weitere Pakete, die für das Experiment benötigt werden.\n",
    "!pip3 install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357d176",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a12bf4da",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Filtering: Datenbereinigung\n",
    "!python3 filter.py Data/Europarl.de-en.en Data/Europarl.de-en.de en de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4cb07",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "b12c0036",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Teilwortmodelle (Subwording Models): Es müssen zwei Teilwortmodelle erstellt werden, um das Vokabular der Quell- und Zielsprache zu lernen.\n",
    "!python3 train.py Data/Europarl.de-en.en-filtered.en Data/Europarl.de-en.de-filtered.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8beef2d",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "2a3802b4",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Tokenisierung: In diesem Schritt werden die Modelle verwendet, die im vorherigen Schritt erzeugt wurden, um das Subwording der Quell- und Zieldateien für die maschinelle Übersetzung durchzuführen\n",
    "!python3 subword.py source.model target.model Data/Europarl.de-en.en-filtered.en Data/Europarl.de-en.de-filtered.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82179827",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "a4f5cd83",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Aufteilung des parallelen Datensatzes in Trainings-, Validierungs- und Testdatensätze für die maschinelle Übersetzung.\n",
    "!python3 train_dev_test_split.py 10000 10000 Data/Europarl.de-en.en-filtered.en.subword Data/Europarl.de-en.de-filtered.de.subword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796dcbc",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "30704948",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Vokabular aufbauen. Mit -n_sample -1 das Vokabular über den gesamten Korpus berechnen.\n",
    "!onmt_build_vocab -config europarl.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fe1ab8",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "bbc850a2",
     "kernelId": "",
     "source_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Training starten.\n",
    "!onmt_train -config europarl.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d405c230",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "a4c319a9",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training ab dem ausgewähltem Checkpoint fortsetzen.\n",
    "!onmt_train -config europarl.yaml -train_from NMT-Model/model_baseline_step_40000.pt -continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf5d8a",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "4e65764a",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Versteckten .trash Ordner auf Paperspace - Gradient löschen\n",
    "# !du --help\n",
    "!rm -rf .Trash-0* # Müll entfernen\n",
    "!du -sch .[!.]** |sort -h # Versteckte Ordner anzeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6afefe",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "588579ba",
     "kernelId": ""
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Übersetzen mit trainiertem Modell.\n",
    "!python translate.py -model NMT-Model/model_baseline_step_40000.pt -src Data/Europarl.de-en.en-filtered.en.subword.test -output Output/pred.txt -replace_unk -verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d930ed1",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "025f2547",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Detokenisierung der maschinellen Übersetzung.\n",
    "# Dieser Schritt ist sinnvoll nachdem das maschinelle Übersetzungsmodell trainiert und Dateien damit übersetzt wurden. Die generierten Zieldateien (d. h. die übersetzten Dateien) müssen detokenisiert werden.\n",
    "!python3 desubword.py target.model Output/pred_batch_size_2k_60k.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b848ef",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "a4a29643",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Detokenisierung der Referenz.\n",
    "!python3 desubword.py target.model Data/Europarl.de-en.de-filtered.de.subword.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa2fd154",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "e964f6c9",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beispiel, Referenz 1. Satz: Dennoch möchte ich darauf hinweisen, daß sich zwar die Situation der Ärzte klärt, dies aber für zahlreiche andere Berufe und Kategorien von Diplomen bei weitem noch nicht der Fall ist.\n",
      "Beispiel, MÜ 1. Satz: Dennoch möchte ich das Hohe Haus daran erinnern, dass der Himmel zwar für den Arzt freundlicher wird, aber bei vielen anderen Berufen und vielen anderen Diplomkategorien bei weitem nicht der Fall ist.\n",
      "BLEU:  29.662547630555782\n"
     ]
    }
   ],
   "source": [
    "# Evaluierung mit BLEU.\n",
    "!python3 compute-bleu.py Data/Europarl.de-en.de-filtered.de.subword.test.desubword Output/pred_batch_size_2k_60k.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d8c52ac",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "baa6aaaa",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "Beendet! Bitte die METEOR Datei 'meteor-Output/pred_batch_size_2k_60k.txt.desubword.txt' im gleichen Verzeichnis prüfen!\n"
     ]
    }
   ],
   "source": [
    "# Evaluierung mit METEOR\n",
    "# Bitte beachten, dass METEOR nur auf Satzebene und nicht auf Korpusebene arbeitet.\n",
    "# Ausgabe ist die Bewertung eines jeden Satzes. Zur Bewertung des Korpus kann anschließend der Mittelwert gebildet werden.\n",
    "!python3 sentence-meteor.py Data/Europarl.de-en.de-filtered.de.subword.test.desubword Output/pred_batch_size_2k_60k.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b56201c",
   "metadata": {
    "collapsed": false,
    "gradient": {
     "editing": false,
     "id": "62571e42",
     "kernelId": "",
     "source_hidden": false
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Evaluierung mit Word Error Rate (WER)\n",
    "!python3 corpus-wer.py Data/Europarl.de-en.de-filtered.de.subword.test.desubword Output/pred_batch_size_2k_60k.txt.desubword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb39bb",
   "metadata": {
    "gradient": {
     "editing": false,
     "id": "da0ed37e",
     "kernelId": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
